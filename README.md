# ğŸ“Œ Complete Spam Classifier Pipeline (MLOps Project)

This repository implements a **modular and production-ready ML pipeline** with proper logging, experiment tracking, and reproducibility using **DVC** (Data Version Control).  

The project is designed around the **Spam Detection Problem** (`spam.csv` dataset) but the pipeline is fully reusable for other ML tasks.  

---

## ğŸš€ Project Structure

```

COMPLETE-ML\_PIPELINE/
â”‚
â”œâ”€â”€ .dvc/                     # DVC metadata
â”œâ”€â”€ data/                     # Raw and processed datasets (tracked by DVC)
â”œâ”€â”€ dvclive/                  # DVC experiment tracking logs
â”œâ”€â”€ env/                      # Virtual environment (not versioned)
â”‚
â”œâ”€â”€ experiments/              # Experiment-related files
â”‚   â”œâ”€â”€ mynotebook.ipynb      # Jupyter notebook for exploration
â”‚   â””â”€â”€ spam.csv              # Dataset
â”‚
â”œâ”€â”€ logs/                     # Logs generated during pipeline execution
â”‚   â”œâ”€â”€ data\_ingestion.log
â”‚   â”œâ”€â”€ data\_preprocessing.log
â”‚   â”œâ”€â”€ feature\_engineering.log
â”‚   â”œâ”€â”€ model\_building.log
â”‚   â””â”€â”€ model\_evaluation.log
â”‚
â”œâ”€â”€ models/                   # Serialized trained models
â”‚   â””â”€â”€ model.pkl
â”‚
â”œâ”€â”€ reports/                  # Model reports & metrics
â”‚   â””â”€â”€ metrics.json
â”‚
â”œâ”€â”€ src/                      # Source code (modular pipeline scripts)
â”‚   â”œâ”€â”€ data\_ingestion.py
â”‚   â”œâ”€â”€ data\_preprocessing.py
â”‚   â”œâ”€â”€ feature\_engineering.py
â”‚   â”œâ”€â”€ model\_building.py
â”‚   â””â”€â”€ model\_evaluation.py
â”‚
â”œâ”€â”€ .dvcignore                # Files ignored by DVC
â”œâ”€â”€ .gitignore                # Files ignored by Git
â”œâ”€â”€ dvc.lock                  # DVC pipeline lock file
â”œâ”€â”€ dvc.yaml                  # DVC pipeline definition
â”œâ”€â”€ LICENSE                   # License file
â”œâ”€â”€ params.yaml               # Parameters for pipeline execution
â””â”€â”€ README.md                 # Project documentation

````

---

## âš™ï¸ Pipeline Stages

This project follows a modular **end-to-end ML pipeline**:

1. **Data Ingestion (`src/data_ingestion.py`)**  
   - Loads raw dataset (`spam.csv`)  
   - Stores dataset in `data/`  
   - Logs details in `logs/data_ingestion.log`

2. **Data Preprocessing (`src/data_preprocessing.py`)**  
   - Cleans text data  
   - Handles missing values, normalization, stopwords, etc.  
   - Logs steps in `logs/data_preprocessing.log`

3. **Feature Engineering (`src/feature_engineering.py`)**  
   - Converts text into numerical features (TF-IDF, Bag of Words, etc.)  
   - Creates training/test splits  
   - Logs details in `logs/feature_engineering.log`

4. **Model Building (`src/model_building.py`)**  
   - Trains ML model (e.g., Logistic Regression, Random Forest)  
   - Saves model in `models/model.pkl`  
   - Logs training details in `logs/model_building.log`

5. **Model Evaluation (`src/model_evaluation.py`)**  
   - Evaluates trained model on test data  
   - Generates metrics (accuracy, precision, recall, F1) in `reports/metrics.json`  
   - Logs results in `logs/model_evaluation.log`

---

## ğŸ“Š Experiment Tracking with DVC

- **DVC** is used to version datasets, models, and pipeline stages.  
- Run the pipeline with:

```bash
dvc repro
````

* Compare experiments:

```bash
dvc metrics show
dvc metrics diff
```

---

## âš¡ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/complete-ml-pipeline.git
cd complete-ml-pipeline
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv env
source env/Scripts/activate  # For Windows
# or
source env/bin/activate      # For Linux/Mac
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Initialize DVC

```bash
dvc init
dvc repro
```

---

## ğŸ“‚ Parameters (`params.yaml`)

All configurable parameters (e.g., dataset paths, model hyperparameters, preprocessing configs) are defined in **params.yaml**.
This allows easy reproducibility and experiment tuning.

Example:

```yaml
data:
  raw_path: data/raw/spam.csv
  processed_path: data/processed/

model:
  type: logistic_regression
  max_iter: 1000
  test_size: 0.2
  random_state: 42
```

---

## ğŸ“‘ Logs

Logs are generated for every stage and stored in `logs/`.
This ensures debugging and auditing capabilities.

Example:

```text
logs/data_ingestion.log
logs/data_preprocessing.log
logs/feature_engineering.log
logs/model_building.log
logs/model_evaluation.log
```

---

## ğŸ“ˆ Reports

* **metrics.json** stores evaluation results in structured format
* Can be compared across runs using `dvc metrics diff`

Example:

```json
{
  "accuracy": 0.95,
  "precision": 0.94,
  "recall": 0.93,
  "f1_score": 0.935
}
```

---

## ğŸ› ï¸ Tech Stack

* **Python**
* **Scikit-learn**
* **Pandas / Numpy**
* **NLTK / Text Processing**
* **DVC (Data Version Control)**
* **Git for Version Control**
* **Logging for Monitoring**

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™Œ Contributions

Contributions, issues, and feature requests are welcome!
Feel free to fork the repo and submit a PR.

```

---

Do you want me to also **generate a `requirements.txt` file** for this pipeline (with sklearn, pandas, numpy, nltk, dvc, etc.)?
```
